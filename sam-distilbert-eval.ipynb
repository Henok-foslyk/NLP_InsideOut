{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SET UP TRAINING DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9833a3d9144dbb973483b396c370ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 2768\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 11\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding, TrainingArguments)\n",
    "from datasets import Dataset, load_dataset, load_metric\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"/Users/samuelwexler/Library/CloudStorage/GoogleDrive-saw9@williams.edu/My Drive/Fall 2024/CSCI 375/Final Project!/eng_train.csv\", split=None)\n",
    "\n",
    "# make vectors for example and labels\n",
    "examples = []\n",
    "y_true = []\n",
    "\n",
    "for example in dataset['train']:\n",
    "  examples.append(example['text'])\n",
    "  y_true.append([example['Anger'], example['Fear'], example['Joy'], example['Sadness'], example['Surprise']])\n",
    "\n",
    "\n",
    "dataset = Dataset.from_dict( {\"text\": examples, \"label\": y_true} )\n",
    "\n",
    "# tokenize examples\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# make tokenized Datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(tokenized_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN WINNING MODEL CONFIGURATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac26555f4434f0db6ba6fde55728543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1038 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4641, 'grad_norm': 2.4478960037231445, 'learning_rate': 1.0366088631984585e-05, 'epoch': 1.45}\n",
      "{'loss': 0.3179, 'grad_norm': 3.5843586921691895, 'learning_rate': 7.321772639691716e-07, 'epoch': 2.89}\n",
      "{'train_runtime': 17670.181, 'train_samples_per_second': 0.47, 'train_steps_per_second': 0.059, 'train_loss': 0.3874825062319952, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type=\"multi_label_classification\", num_labels=5)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", \n",
    "                                  per_device_train_batch_size=8, \n",
    "                                  learning_rate=2e-5)\n",
    "trainer = Trainer(model=model, \n",
    "                  args=training_args, \n",
    "                  train_dataset=tokenized_dataset)\n",
    "trainer.train()\n",
    "trainer.save_model(\"/Users/samuelwexler/Documents/CS375/final-project\")\n",
    "\n",
    "\n",
    "# eventually np.savetxt('my_array.txt', arr, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INGEST DEV DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id                                               text  \\\n",
      "0    eng_dev_track_a_00001            My mouth fell open `` No, no, no... I..   \n",
      "1    eng_dev_track_a_00002  You can barely make out your daughter's pale f...   \n",
      "2    eng_dev_track_a_00003  But after blinking my eyes for a few times lep...   \n",
      "3    eng_dev_track_a_00004  Slowly rising to my feet I came to the conclus...   \n",
      "4    eng_dev_track_a_00005  I noticed this months after moving in and doin...   \n",
      "..                     ...                                                ...   \n",
      "111  eng_dev_track_a_00112                       \"ARcH stop your progression.   \n",
      "112  eng_dev_track_a_00113        This 'star', starts to move across the sky.   \n",
      "113  eng_dev_track_a_00114                                  and my feet hurt.   \n",
      "114  eng_dev_track_a_00115        so i cried my eyes out and did the drawing.   \n",
      "115  eng_dev_track_a_00116                              They were coal black.   \n",
      "\n",
      "     Anger  Fear  Joy  Sadness  Surprise  \n",
      "0      NaN   NaN  NaN      NaN       NaN  \n",
      "1      NaN   NaN  NaN      NaN       NaN  \n",
      "2      NaN   NaN  NaN      NaN       NaN  \n",
      "3      NaN   NaN  NaN      NaN       NaN  \n",
      "4      NaN   NaN  NaN      NaN       NaN  \n",
      "..     ...   ...  ...      ...       ...  \n",
      "111    NaN   NaN  NaN      NaN       NaN  \n",
      "112    NaN   NaN  NaN      NaN       NaN  \n",
      "113    NaN   NaN  NaN      NaN       NaN  \n",
      "114    NaN   NaN  NaN      NaN       NaN  \n",
      "115    NaN   NaN  NaN      NaN       NaN  \n",
      "\n",
      "[116 rows x 7 columns]\n",
      "(116, 7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4d2b72726d49b08ad82117cc62c54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I would go over to his house where his Grandmother and mom and dad lived to play SSX Tricky and eat Pokemon fruit rollups.', 'input_ids': [101, 1045, 2052, 2175, 2058, 2000, 2010, 2160, 2073, 2010, 7133, 1998, 3566, 1998, 3611, 2973, 2000, 2377, 7020, 2595, 24026, 1998, 4521, 20421, 5909, 4897, 22264, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "dev_set = pd.read_csv(\"/Users/samuelwexler/Library/CloudStorage/GoogleDrive-saw9@williams.edu/My Drive/Fall 2024/CSCI 375/Final Project!/public_data/dev/track_a/eng_a.csv\")\n",
    "\n",
    "print(dev_set)\n",
    "print(dev_set.shape)\n",
    "\n",
    "dev_examples = []\n",
    "\n",
    "for i in range(len(dev_set)):\n",
    "    dev_examples.append(dev_set.iloc[i]['text'])\n",
    "\n",
    "dev_dataset = Dataset.from_dict( {\"text\": dev_examples} )\n",
    "tokenized_dev = dev_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_dev[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICT ENGLISH DEV SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed62e2a44af249418aa513ae69fd6ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBS:  tensor([[0.1959, 0.9371, 0.0266, 0.2787, 0.6651],\n",
      "        [0.0711, 0.9479, 0.0222, 0.5588, 0.5997],\n",
      "        [0.0202, 0.1544, 0.5521, 0.0419, 0.1116],\n",
      "        [0.0272, 0.5158, 0.1499, 0.5890, 0.0312],\n",
      "        [0.0276, 0.1228, 0.6902, 0.0289, 0.3776],\n",
      "        [0.0578, 0.4569, 0.0769, 0.2821, 0.0215],\n",
      "        [0.8338, 0.5886, 0.1299, 0.4505, 0.2976],\n",
      "        [0.0377, 0.4901, 0.0704, 0.3530, 0.0598],\n",
      "        [0.0226, 0.1588, 0.6369, 0.0378, 0.0701],\n",
      "        [0.0685, 0.9365, 0.0323, 0.6514, 0.0443],\n",
      "        [0.0406, 0.0887, 0.8627, 0.0356, 0.2080],\n",
      "        [0.0888, 0.8934, 0.0406, 0.8014, 0.0328],\n",
      "        [0.4077, 0.4699, 0.0632, 0.6350, 0.1636],\n",
      "        [0.4182, 0.9228, 0.0638, 0.2436, 0.8500],\n",
      "        [0.0327, 0.0722, 0.8159, 0.1231, 0.1041],\n",
      "        [0.0163, 0.5397, 0.0930, 0.2893, 0.1580],\n",
      "        [0.7857, 0.7473, 0.0673, 0.6199, 0.2477],\n",
      "        [0.0127, 0.4719, 0.2078, 0.0344, 0.2820],\n",
      "        [0.2595, 0.9371, 0.0447, 0.1402, 0.6726],\n",
      "        [0.7496, 0.7949, 0.0614, 0.2952, 0.2301],\n",
      "        [0.0686, 0.2909, 0.6465, 0.1047, 0.8786],\n",
      "        [0.0323, 0.8803, 0.0388, 0.3394, 0.0430],\n",
      "        [0.1050, 0.6123, 0.0596, 0.2618, 0.5573],\n",
      "        [0.8375, 0.8418, 0.0955, 0.5063, 0.5709],\n",
      "        [0.1411, 0.9352, 0.0215, 0.5517, 0.2501],\n",
      "        [0.0417, 0.1647, 0.2352, 0.0964, 0.5929],\n",
      "        [0.0690, 0.9716, 0.0239, 0.3529, 0.4060],\n",
      "        [0.0340, 0.0698, 0.7542, 0.0382, 0.1785],\n",
      "        [0.0391, 0.0715, 0.8676, 0.1283, 0.1075],\n",
      "        [0.0466, 0.7365, 0.1517, 0.0397, 0.8732],\n",
      "        [0.0523, 0.8834, 0.0344, 0.5593, 0.0404],\n",
      "        [0.0775, 0.6037, 0.1417, 0.0403, 0.2131],\n",
      "        [0.4590, 0.1997, 0.3918, 0.1056, 0.1329],\n",
      "        [0.6438, 0.7445, 0.0905, 0.7376, 0.1541],\n",
      "        [0.0531, 0.5151, 0.1914, 0.0400, 0.0717],\n",
      "        [0.0312, 0.7588, 0.0918, 0.1062, 0.1137],\n",
      "        [0.0425, 0.7509, 0.0607, 0.3072, 0.0224],\n",
      "        [0.1182, 0.9342, 0.0386, 0.8894, 0.1007],\n",
      "        [0.0743, 0.0689, 0.9354, 0.0803, 0.2046],\n",
      "        [0.0281, 0.1165, 0.6134, 0.1213, 0.0409],\n",
      "        [0.0311, 0.1044, 0.8436, 0.0445, 0.1019],\n",
      "        [0.0260, 0.3774, 0.1718, 0.0488, 0.2591],\n",
      "        [0.3143, 0.9520, 0.0216, 0.6596, 0.1503],\n",
      "        [0.0328, 0.2308, 0.4523, 0.0418, 0.0699],\n",
      "        [0.7868, 0.7953, 0.0605, 0.3627, 0.2756],\n",
      "        [0.0309, 0.9440, 0.0278, 0.4690, 0.2718],\n",
      "        [0.0512, 0.7439, 0.0289, 0.4109, 0.1061],\n",
      "        [0.0194, 0.7947, 0.0697, 0.0603, 0.7187],\n",
      "        [0.1394, 0.7792, 0.0473, 0.8920, 0.0403],\n",
      "        [0.0388, 0.7228, 0.0498, 0.1981, 0.0334],\n",
      "        [0.1254, 0.7494, 0.0630, 0.9070, 0.0641],\n",
      "        [0.3364, 0.9619, 0.0366, 0.4093, 0.7340],\n",
      "        [0.1012, 0.8504, 0.0321, 0.1800, 0.4840],\n",
      "        [0.2084, 0.8731, 0.0292, 0.4018, 0.4772],\n",
      "        [0.0392, 0.2518, 0.4251, 0.0201, 0.4812],\n",
      "        [0.1032, 0.9436, 0.0247, 0.6102, 0.0807],\n",
      "        [0.2003, 0.3938, 0.1049, 0.4758, 0.0343],\n",
      "        [0.0256, 0.3036, 0.3749, 0.0548, 0.7408],\n",
      "        [0.4391, 0.1679, 0.5519, 0.1165, 0.4593],\n",
      "        [0.0686, 0.4700, 0.1802, 0.8463, 0.0391],\n",
      "        [0.0278, 0.1007, 0.6811, 0.0556, 0.0686],\n",
      "        [0.1814, 0.9483, 0.0263, 0.4028, 0.7380],\n",
      "        [0.0330, 0.5842, 0.1134, 0.0740, 0.0536],\n",
      "        [0.0379, 0.9138, 0.0393, 0.1790, 0.6461],\n",
      "        [0.0237, 0.2475, 0.7425, 0.0339, 0.1335],\n",
      "        [0.0849, 0.8325, 0.0501, 0.7053, 0.0209],\n",
      "        [0.0368, 0.8740, 0.0443, 0.1549, 0.0644],\n",
      "        [0.0595, 0.7275, 0.1102, 0.0484, 0.8891],\n",
      "        [0.0836, 0.7815, 0.0424, 0.7284, 0.0251],\n",
      "        [0.0785, 0.9558, 0.0261, 0.5619, 0.0701],\n",
      "        [0.0521, 0.0688, 0.7799, 0.0629, 0.0769],\n",
      "        [0.0351, 0.1419, 0.6336, 0.0287, 0.2799],\n",
      "        [0.0818, 0.7699, 0.0837, 0.0940, 0.9006],\n",
      "        [0.0406, 0.2789, 0.6807, 0.0291, 0.7072],\n",
      "        [0.0587, 0.8398, 0.0566, 0.8338, 0.2805],\n",
      "        [0.1030, 0.1371, 0.7460, 0.0292, 0.3129],\n",
      "        [0.1394, 0.1344, 0.8214, 0.0787, 0.5955],\n",
      "        [0.0463, 0.1208, 0.6002, 0.0473, 0.5872],\n",
      "        [0.0361, 0.1040, 0.6102, 0.0734, 0.0504],\n",
      "        [0.0540, 0.9450, 0.0298, 0.8027, 0.1085],\n",
      "        [0.0935, 0.9458, 0.0302, 0.7523, 0.0559],\n",
      "        [0.1170, 0.9748, 0.0272, 0.5160, 0.4992],\n",
      "        [0.0324, 0.5478, 0.1026, 0.1621, 0.0528],\n",
      "        [0.0257, 0.3514, 0.1414, 0.1380, 0.6991],\n",
      "        [0.1680, 0.8739, 0.0467, 0.9042, 0.2904],\n",
      "        [0.0674, 0.1211, 0.9027, 0.0515, 0.5294],\n",
      "        [0.1664, 0.6088, 0.0827, 0.9017, 0.1566],\n",
      "        [0.4113, 0.2217, 0.3067, 0.3075, 0.0598],\n",
      "        [0.0763, 0.6502, 0.0630, 0.8253, 0.0310],\n",
      "        [0.0595, 0.9130, 0.0347, 0.3943, 0.0465],\n",
      "        [0.2178, 0.8404, 0.0225, 0.2794, 0.5888],\n",
      "        [0.1212, 0.8680, 0.0525, 0.1099, 0.7871],\n",
      "        [0.2574, 0.9492, 0.0212, 0.5115, 0.5715],\n",
      "        [0.8447, 0.6580, 0.1120, 0.5049, 0.1668],\n",
      "        [0.0362, 0.6979, 0.1230, 0.1651, 0.0416],\n",
      "        [0.5974, 0.8762, 0.0733, 0.2784, 0.8765],\n",
      "        [0.0155, 0.7126, 0.1460, 0.0443, 0.3242],\n",
      "        [0.0549, 0.9255, 0.0368, 0.3824, 0.0418],\n",
      "        [0.0345, 0.2286, 0.3200, 0.0354, 0.0623],\n",
      "        [0.0326, 0.8310, 0.1232, 0.0405, 0.6176],\n",
      "        [0.0275, 0.2841, 0.3616, 0.0183, 0.2346],\n",
      "        [0.0160, 0.5521, 0.2181, 0.0509, 0.1005],\n",
      "        [0.2250, 0.1434, 0.5231, 0.0912, 0.0617],\n",
      "        [0.0284, 0.8936, 0.0245, 0.2793, 0.3856],\n",
      "        [0.0164, 0.1637, 0.4123, 0.0495, 0.1365],\n",
      "        [0.0242, 0.1573, 0.7830, 0.0311, 0.4045],\n",
      "        [0.0571, 0.5463, 0.0778, 0.7431, 0.0340],\n",
      "        [0.0280, 0.3360, 0.1813, 0.2668, 0.0226],\n",
      "        [0.1563, 0.6288, 0.1023, 0.8636, 0.0312],\n",
      "        [0.6421, 0.9018, 0.0366, 0.6398, 0.3443],\n",
      "        [0.0239, 0.4046, 0.1192, 0.0702, 0.0616],\n",
      "        [0.2621, 0.8282, 0.0394, 0.0984, 0.5104],\n",
      "        [0.0363, 0.5142, 0.3706, 0.0442, 0.8557],\n",
      "        [0.0609, 0.8834, 0.0420, 0.5629, 0.0254],\n",
      "        [0.0695, 0.4378, 0.1224, 0.8341, 0.0423],\n",
      "        [0.0501, 0.4403, 0.0534, 0.1986, 0.5572]])\n",
      "tensor([[0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [1, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [1, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 1, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_dev) # logits\n",
    "probs = torch.sigmoid(torch.from_numpy(predictions.predictions)) # percentage probabilities\n",
    "print(\"PROBS: \", probs)\n",
    "binary_predictions = (probs >= 0.35).long()\n",
    "print(binary_predictions)\n",
    "\n",
    "# Convert tensors to integers\n",
    "binary_predictions_list = binary_predictions.tolist()\n",
    "\n",
    "\n",
    "ids = []\n",
    "for i in range(0, len(binary_predictions_list)):\n",
    "  ids.append(dev_set['id'][i])\n",
    "\n",
    "data = {\n",
    "    \"id\": ids,\n",
    "    \"Anger\": [pred[0] for pred in binary_predictions_list],\n",
    "    \"Fear\": [pred[1] for pred in binary_predictions_list],\n",
    "    \"Joy\": [pred[2] for pred in binary_predictions_list],\n",
    "    \"Sadness\": [pred[3] for pred in binary_predictions_list],\n",
    "    \"Surprise\": [pred[4] for pred in binary_predictions_list],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"pred_eng_a.csv\", index=False) # drop the index column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INGEST (TRANSLATED) AMHARIC DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690d9815c0114cc0b8df973ddbd3a373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3549 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 3549\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "amh_source = load_dataset(\"csv\", data_files=\"/Users/samuelwexler/Library/CloudStorage/GoogleDrive-saw9@williams.edu/My Drive/Fall 2024/CSCI 375/Final Project!/amh_translated_ordered.csv\", split=None)\n",
    "\n",
    "# make vectors for example and labels\n",
    "amh_examples = []\n",
    "amh_y_true = []\n",
    "\n",
    "for example in amh_source['train']:\n",
    "  amh_examples.append(example['text'])\n",
    "  amh_y_true.append([example['Anger'], example['Fear'], example['Joy'], example['Sadness'], example['Surprise']])\n",
    "\n",
    "\n",
    "amh_dataset = Dataset.from_dict( {\"text\": amh_examples, \"label\": amh_y_true} )\n",
    "\n",
    "# make tokenized Datasets\n",
    "amh_tokenized_dataset = amh_dataset.map(tokenize_function, batched=True)\n",
    "print(amh_tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST ON (TRANSLATED) AMHARIC SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a36f55c4c442f1b3ddd119f41d5e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4970749049665876\n",
      "0.3663958575127221\n",
      "0.359234257804785\n"
     ]
    }
   ],
   "source": [
    "amh_predictions = trainer.predict(amh_tokenized_dataset) # logits\n",
    "amh_probs = torch.sigmoid(torch.from_numpy(amh_predictions.predictions)) # percentage probabilities\n",
    "amh_binary_predictions = (amh_probs >= 0.35).long()\n",
    "\n",
    "print(f1_score(y_true=amh_y_true, y_pred=amh_binary_predictions, average='weighted'))\n",
    "print(f1_score(y_true=amh_y_true, y_pred=amh_binary_predictions, average='micro'))\n",
    "print(f1_score(y_true=amh_y_true, y_pred=amh_binary_predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INGEST HELSINKI SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b02f315678549cb8317c751735b7235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12243 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 12243\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hel_source = load_dataset(\"csv\", data_files=\"/Users/samuelwexler/Library/CloudStorage/GoogleDrive-saw9@williams.edu/My Drive/Fall 2024/CSCI 375/Final Project!/xed_fixed.csv\", split=None)\n",
    "\n",
    "# make vectors for example and labels\n",
    "hel_examples = []\n",
    "hel_y_true = []\n",
    "\n",
    "for example in hel_source['train']:\n",
    "  hel_examples.append(example['Sentence'])\n",
    "  hel_y_true.append([example['Anger'], example['Fear'], example['Joy'], example['Sadness'], example['Surprise']])\n",
    "\n",
    "\n",
    "hel_dataset = Dataset.from_dict( {\"text\": hel_examples, \"label\": hel_y_true} )\n",
    "\n",
    "# make tokenized Datasets\n",
    "hel_tokenized_dataset = hel_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# print(tokenized_dataset[0])\n",
    "print(hel_tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICT ON HELSINKI SET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acd2c2b910c480fa845285cded5a311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5178960298916008\n",
      "0.5009111797797322\n",
      "0.5123829961601433\n"
     ]
    }
   ],
   "source": [
    "hel_predictions = trainer.predict(hel_tokenized_dataset) # logits\n",
    "hel_probs = torch.sigmoid(torch.from_numpy(hel_predictions.predictions)) # percentage probabilities\n",
    "hel_binary_predictions = (hel_probs >= 0.35).long()\n",
    "\n",
    "print(f1_score(y_true=hel_y_true, y_pred=hel_binary_predictions, average='weighted'))\n",
    "print(f1_score(y_true=hel_y_true, y_pred=hel_binary_predictions, average='micro'))\n",
    "print(f1_score(y_true=hel_y_true, y_pred=hel_binary_predictions, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs375",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
