{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for knowledgator/events_classification_biotech contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/knowledgator/events_classification_biotech\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['event organization', 'executive statement', 'regulatory approval', 'hiring', 'foundation', 'closing', 'partnerships & alliances', 'expanding industry', 'new initiatives or programs', 'm&a', 'service & product providing', 'new initiatives & programs', 'subsidiary establishment', 'product launching & presentation', 'product updates', 'executive appointment', 'alliance & partnership', 'ipo exit', 'article publication', 'clinical trial sponsorship', 'company description', 'investment in public company', 'other', 'expanding geography', 'participation in an event', 'support & philanthropy', 'department establishment', 'funding round', 'patent publication']\n"
     ]
    }
   ],
   "source": [
    "# Environment check \n",
    "# Return to HW0 if you run into errors in this cell \n",
    "# Do not modify this cell \n",
    "import os\n",
    "# assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 11\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          TrainingArguments, Trainer, DataCollatorWithPadding, TrainingArguments)\n",
    "from datasets import Dataset, load_dataset, load_metric\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "dataset = load_dataset('csv', data_files=r'train\\track_a\\eng.csv')\n",
    "dataset2 = load_dataset('knowledgator/events_classification_biotech')\n",
    "\n",
    "dev_dataset = load_dataset('csv', data_files=r'dev\\track_a\\eng_a.csv')\n",
    "\n",
    "helinski_dataset_raw = load_dataset('csv', data_files=r'xed_fixed.csv')\n",
    "\n",
    "classes2 = [class_ for class_ in dataset2['train'].features['label 1'].names if class_]\n",
    "print(classes2)\n",
    "\n",
    "# class_weights = torch.tensor([1.8, 0.7, 1.2, 0.9, 0.9])\n",
    "\n",
    "# loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "classes = [\"Anger\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\"]\n",
    "class2id2 = {class_:id for id, class_ in enumerate(classes2)}\n",
    "class2id = {class_:id for id, class_ in enumerate(classes)}\n",
    "id2class = {id:class_ for class_, id in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    " \n",
    "model_path = 'microsoft/deberta-v3-small'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608e4e430a2a41ad9ec7606993861810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2629 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6573faf38f3456abccbd55cf854bba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/139 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f118b77aea1b4e238eb72c1788c077f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/116 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3104435dd7be44ddb7ab5d8c63f736c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12243 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def preprocess_function(example):\n",
    "#    text = example['text']\n",
    "#    labels = [0. for i in range(len(classes))]\n",
    "#    for label in classes:\n",
    "#        id = class2id[label]\n",
    "#        labels[id] = float(example[label])\n",
    "  \n",
    "#    example = tokenizer(text, truncation=True)\n",
    "#    example['labels'] = labels\n",
    "#    return example\n",
    "\n",
    "examples = []\n",
    "y_true = []\n",
    "dev_examples = []\n",
    "dev_y_true = []\n",
    "h_examples = []\n",
    "h_y_true = []\n",
    "num_folds = 5\n",
    "\n",
    "for example in dataset['train']:\n",
    "  examples.append(example['text'])\n",
    "  y_true.append([float(example['Anger']), float(example['Fear']), float(example['Joy']), float(example['Sadness']), float(example['Surprise'])])\n",
    "\n",
    "for example in dev_dataset['train']:\n",
    "  dev_examples.append(example['text'])\n",
    "  # dev_y_true.append([float(example['Anger']), float(example['Fear']), float(example['Joy']), float(example['Sadness']), float(example['Surprise'])])\n",
    "\n",
    "for example in helinski_dataset_raw['train']:\n",
    "  h_examples.append(example['Sentence'])\n",
    "  h_y_true.append([float(example['Anger']), float(example['Fear']), float(example['Joy']), float(example['Sadness']), float(example['Surprise'])])\n",
    "\n",
    "# make training and validation sets\n",
    "examples_train, examples_test, labels_train, labels_test = train_test_split(examples, y_true, test_size=0.05, random_state=42)\n",
    "\n",
    "# print(examples_train)\n",
    "# print(examples_test)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict( {\"text\": examples_train, \"label\": labels_train} )\n",
    "test_dataset = Dataset.from_dict( {\"text\": examples_test, \"label\": labels_test} )\n",
    "helinski_dataset = Dataset.from_dict( {\"text\": h_examples, \"label\": h_y_true} )\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "new_dataset = Dataset.from_dict( {\"text\": examples, \"label\": y_true} )\n",
    "dev_dataset = Dataset.from_dict( {\"text\": dev_examples} )\n",
    "\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=num_folds)\n",
    "# kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "accumulated_trains = []\n",
    "accumulated_tests = []\n",
    "\n",
    "  # make Datasets\n",
    "# train = new_dataset.select(train_index)\n",
    "# tokenizd_train = new_dataset.map(tokenize_function, batched=True)\n",
    "# accumulated_trains.append(tokenizd_train)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_traintest = test_dataset.map(tokenize_function, batched=True)\n",
    "# test = new_dataset.select(val_index)\n",
    "tokenized_dev = dev_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_helinski = helinski_dataset.map(tokenize_function, batched=True)\n",
    "# accumulated_tests.append(tokenized_test)\n",
    "\n",
    "# tokenized_test = test_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def preprocess_function(example):\n",
    "# #    text = example['text']\n",
    "# #    labels = [0. for i in range(len(classes))]\n",
    "# #    for label in classes:\n",
    "# #        id = class2id[label]\n",
    "# #        labels[id] = float(example[label])\n",
    "  \n",
    "# #    example = tokenizer(text, truncation=True)\n",
    "# #    example['labels'] = labels\n",
    "# #    return example\n",
    "\n",
    "# examples = []\n",
    "# y_true = []\n",
    "# num_folds = 5\n",
    "\n",
    "# for example in dataset['train']:\n",
    "#   examples.append(example['text'])\n",
    "#   y_true.append([float(example['Anger']), float(example['Fear']), float(example['Joy']), float(example['Sadness']), float(example['Surprise'])])\n",
    "\n",
    "# # make training and validation sets\n",
    "# # examples_train, examples_test, labels_train, labels_test = train_test_split(examples, y_true, test_size=0.05, random_state=42)\n",
    "\n",
    "# # print(examples_train)\n",
    "# # print(examples_test)\n",
    "\n",
    "\n",
    "# # train_dataset = Dataset.from_dict( {\"text\": examples_train, \"label\": labels_train} )\n",
    "# # test_dataset = Dataset.from_dict( {\"text\": examples_test, \"label\": labels_test} )\n",
    "\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# new_dataset = Dataset.from_dict( {\"text\": examples, \"label\": y_true} )\n",
    "\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=num_folds)\n",
    "# # kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# accumulated_trains = []\n",
    "# accumulated_tests = []\n",
    "\n",
    "# for fold, (train_index, val_index) in enumerate(mskf.split(examples, y_true)):\n",
    "#   # make Datasets\n",
    "#   train = new_dataset.select(train_index)\n",
    "#   tokenizd_train = train.map(tokenize_function, batched=True)\n",
    "#   accumulated_trains.append(tokenizd_train)\n",
    "  \n",
    "#   test = new_dataset.select(val_index)\n",
    "#   tokenized_test = test.map(tokenize_function, batched=True)\n",
    "#   accumulated_tests.append(tokenized_test)\n",
    "\n",
    "# # tokenized_test = test_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# print(tokenized_dataset)\n",
    "# tokenized_dataset = tokenized_dataset[\"train\"].train_test_split(test_size=0.6) \n",
    "# print(tokenized_dataset)\n",
    "# training!\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, problem_type=\"multi_label_classification\", num_labels=5)\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\n",
    "\n",
    "# mini_tokenized_train = tokenized_train.select(range(100))\n",
    "# mini_tokenized_test = tokenized_test.select(range(5))\n",
    "\n",
    "#  trainer = Trainer(model=model, args=training_args, train_dataset=mini_tokenized_train, eval_dataset=mini_tokenized_test)\n",
    "# trainer = CustomTrainer(model=model, args=training_args, train_dataset=mini_tokenized_train, eval_dataset=mini_tokenized_test)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def sigmoid(x):\n",
    "   return 1/(1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "   predictions, labels = eval_pred\n",
    "   predictions = sigmoid(predictions)\n",
    "   predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "   return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))\n",
    "#references=labels.astype(int).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "\n",
    "   model_path, num_labels=len(classes),\n",
    "           id2label=id2class, label2id=class2id,\n",
    "                       problem_type = \"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa1656647b142e3bf4533c10429c551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/987 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f65355c477a44179ca3bc16e8a9f383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45140814781188965, 'eval_accuracy': 0.7755395683453238, 'eval_f1': 0.6231884057971014, 'eval_precision': 0.6417910447761194, 'eval_recall': 0.6056338028169014, 'eval_runtime': 3.6509, 'eval_samples_per_second': 38.073, 'eval_steps_per_second': 4.93, 'epoch': 1.0}\n",
      "{'loss': 0.4872, 'grad_norm': 3.015688896179199, 'learning_rate': 9.868287740628168e-06, 'epoch': 1.52}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e241bda14bf4738a29577bd8141a002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4059754014015198, 'eval_accuracy': 0.8100719424460432, 'eval_f1': 0.6432432432432432, 'eval_precision': 0.7579617834394905, 'eval_recall': 0.5586854460093896, 'eval_runtime': 2.8121, 'eval_samples_per_second': 49.43, 'eval_steps_per_second': 6.401, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8aca092544478ab5467b7fdf551346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3965293765068054, 'eval_accuracy': 0.8244604316546763, 'eval_f1': 0.695, 'eval_precision': 0.7433155080213903, 'eval_recall': 0.6525821596244131, 'eval_runtime': 3.4309, 'eval_samples_per_second': 40.515, 'eval_steps_per_second': 5.246, 'epoch': 3.0}\n",
      "{'train_runtime': 1771.2377, 'train_samples_per_second': 4.453, 'train_steps_per_second': 0.557, 'train_loss': 0.4209353554212576, 'epoch': 3.0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 31\u001b[0m\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m    model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m    \u001b[38;5;66;03m# mini_batch_sizee = b_s\u001b[39;00m\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 31\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(TESTING_SET)\n\u001b[0;32m     32\u001b[0m probs \u001b[38;5;241m=\u001b[39m sigmoid(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(predictions\u001b[38;5;241m.\u001b[39mpredictions))\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# print(\"PROBS: \", probs)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# print(\"LABELS: \", torch.tensor(tokenized_test['label'])) # trues\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\trainer.py:3744\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3741\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3743\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3744\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[0;32m   3745\u001b[0m     test_dataloader, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix\n\u001b[0;32m   3746\u001b[0m )\n\u001b[0;32m   3747\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\trainer.py:3847\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3844\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3846\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 3847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m   3848\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[0;32m   3849\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[0;32m   3850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\accelerate\\data_loader.py:552\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 552\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\data\\data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m pad_without_fast_tokenizer_warning(\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m    273\u001b[0m         features,\n\u001b[0;32m    274\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[0;32m    275\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m    276\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    277\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_tensors,\n\u001b[0;32m    278\u001b[0m     )\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3479\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[0;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 3479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3481\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3482\u001b[0m     )\n\u001b[0;32m   3484\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m   3486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']"
     ]
    }
   ],
   "source": [
    "# for i in range(num_folds):\n",
    "\n",
    "TESTING_SET = tokenized_helinski\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"my_awesome_model\",\n",
    "   per_device_train_batch_size = 8,\n",
    "   learning_rate= 2e-5,\n",
    "   # per_device_train_batch_size=3,\n",
    "   # per_device_eval_batch_size=3,\n",
    "   num_train_epochs=3,\n",
    "   weight_decay=0.01,\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_traintest,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    "   # mini_batch_sizee = b_s\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(TESTING_SET)\n\u001b[0;32m      2\u001b[0m probs \u001b[38;5;241m=\u001b[39m sigmoid(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(predictions\u001b[38;5;241m.\u001b[39mpredictions))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print(\"PROBS: \", probs)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# print(\"LABELS: \", torch.tensor(tokenized_test['label'])) # trues\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\trainer.py:3744\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3741\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3743\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3744\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[0;32m   3745\u001b[0m     test_dataloader, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys, metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix\n\u001b[0;32m   3746\u001b[0m )\n\u001b[0;32m   3747\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\trainer.py:3847\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3844\u001b[0m observed_num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3846\u001b[0m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[1;32m-> 3847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m   3848\u001b[0m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[0;32m   3849\u001b[0m     observed_batch_size \u001b[38;5;241m=\u001b[39m find_batch_size(inputs)\n\u001b[0;32m   3850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\accelerate\\data_loader.py:552\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 552\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\data\\data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m pad_without_fast_tokenizer_warning(\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[0;32m    273\u001b[0m         features,\n\u001b[0;32m    274\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[0;32m    275\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m    276\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    277\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_tensors,\n\u001b[0;32m    278\u001b[0m     )\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\data\\data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32mc:\\Users\\thato\\miniconda3\\envs\\semeval1\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3479\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[0;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 3479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3481\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(encoded_inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3482\u001b[0m     )\n\u001b[0;32m   3484\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m   3486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(TESTING_SET)\n",
    "probs = sigmoid(torch.from_numpy(predictions.predictions))\n",
    "# print(\"PROBS: \", probs)\n",
    "# print(\"LABELS: \", torch.tensor(tokenized_test['label'])) # trues\n",
    "print(f\"RESULTS LOG:\")\n",
    "nums = np.round(np.linspace(.3, .7, 40), 2)\n",
    "final_nums = np.append(nums, 0.5)\n",
    "\n",
    "best_thresh = 0\n",
    "best_f1 = 0\n",
    "for curr_thresh in final_nums:\n",
    "# binarize predictions\n",
    "   binary_predictions = (probs >= curr_thresh).long()\n",
    "   \n",
    "   # print(\"PREDS: \", binary_predictions)\n",
    "\n",
    "   curr_f1 = f1_score(y_true=TESTING_SET['label'], y_pred=binary_predictions, average='weighted')\n",
    "   if curr_f1 > best_f1:\n",
    "      best_f1 = curr_f1\n",
    "      best_thresh = curr_thresh\n",
    "# print(\"THRESH = \", curr_thresh, \" F1 = \", curr_f1)\n",
    "print(f\"F1: {best_f1}\")\n",
    "print(best_thresh)\n",
    "\n",
    "for thresh in [0.35, 0.4, 0.45, 0.5]:\n",
    "# binarize predictions\n",
    "   binary_predictions = (probs >= thresh).long()\n",
    "   print(\"THRESH = \", thresh)\n",
    "   # print(\"PREDS: \", binary_predictions)\n",
    "\n",
    "   print(f1_score(y_true=TESTING_SET['label'], y_pred=binary_predictions, average='weighted'))\n",
    "   print(f1_score(y_true=TESTING_SET['label'], y_pred=binary_predictions, average='micro'))\n",
    "   print(f1_score(y_true=TESTING_SET['label'], y_pred=binary_predictions, average='macro'))\n",
    "   print(\"==============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f3f7407dcd4bb681eff833aab3b205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROBS:  tensor([[0.3542, 0.8581, 0.0265, 0.3875, 0.2405],\n",
      "        [0.0391, 0.9395, 0.0265, 0.2670, 0.6871],\n",
      "        [0.0251, 0.5663, 0.1758, 0.0363, 0.5725],\n",
      "        [0.0177, 0.3833, 0.2688, 0.3096, 0.0226],\n",
      "        [0.0162, 0.2484, 0.3311, 0.0590, 0.2025],\n",
      "        [0.0339, 0.7683, 0.0616, 0.4642, 0.0199],\n",
      "        [0.8415, 0.5247, 0.0939, 0.6800, 0.2348],\n",
      "        [0.0435, 0.2545, 0.3271, 0.2512, 0.0394],\n",
      "        [0.0163, 0.1667, 0.5121, 0.0633, 0.0428],\n",
      "        [0.1212, 0.9230, 0.0231, 0.7243, 0.0340],\n",
      "        [0.0389, 0.1058, 0.7152, 0.0519, 0.3301],\n",
      "        [0.0346, 0.9215, 0.0334, 0.7368, 0.0332],\n",
      "        [0.5502, 0.6205, 0.0742, 0.3807, 0.4905],\n",
      "        [0.2147, 0.8655, 0.0323, 0.1979, 0.6419],\n",
      "        [0.0480, 0.0566, 0.9024, 0.1026, 0.1354],\n",
      "        [0.0129, 0.7609, 0.0906, 0.2587, 0.1066],\n",
      "        [0.5753, 0.4848, 0.1038, 0.6517, 0.1471],\n",
      "        [0.0122, 0.5390, 0.1755, 0.0418, 0.2172],\n",
      "        [0.2505, 0.9259, 0.0249, 0.2464, 0.5295],\n",
      "        [0.5102, 0.7715, 0.0425, 0.6519, 0.0894],\n",
      "        [0.0494, 0.0726, 0.8605, 0.0840, 0.2560],\n",
      "        [0.0271, 0.8809, 0.0375, 0.4428, 0.0366],\n",
      "        [0.8035, 0.5028, 0.0988, 0.5603, 0.3178],\n",
      "        [0.7586, 0.7712, 0.0535, 0.4321, 0.4757],\n",
      "        [0.0543, 0.9176, 0.0217, 0.2793, 0.1757],\n",
      "        [0.1025, 0.7216, 0.0763, 0.0992, 0.7834],\n",
      "        [0.0651, 0.9513, 0.0234, 0.1891, 0.6649],\n",
      "        [0.0512, 0.0592, 0.8053, 0.0675, 0.1604],\n",
      "        [0.0390, 0.0568, 0.8888, 0.1227, 0.1032],\n",
      "        [0.0271, 0.2563, 0.4190, 0.0381, 0.5258],\n",
      "        [0.0192, 0.5175, 0.1467, 0.1444, 0.0339],\n",
      "        [0.0193, 0.3632, 0.2572, 0.0842, 0.0496],\n",
      "        [0.7246, 0.6083, 0.0757, 0.4529, 0.4484],\n",
      "        [0.7878, 0.6090, 0.0841, 0.7913, 0.1364],\n",
      "        [0.0196, 0.6896, 0.0894, 0.0690, 0.1858],\n",
      "        [0.0396, 0.8517, 0.0371, 0.2214, 0.1072],\n",
      "        [0.0142, 0.4854, 0.2204, 0.2386, 0.0179],\n",
      "        [0.1104, 0.9442, 0.0217, 0.8464, 0.0528],\n",
      "        [0.0609, 0.0543, 0.9280, 0.1014, 0.1223],\n",
      "        [0.0222, 0.1353, 0.6086, 0.1552, 0.0315],\n",
      "        [0.0161, 0.2046, 0.4473, 0.1022, 0.0528],\n",
      "        [0.0367, 0.7350, 0.0671, 0.0843, 0.2508],\n",
      "        [0.3239, 0.9311, 0.0202, 0.7871, 0.0831],\n",
      "        [0.0300, 0.0663, 0.7875, 0.0624, 0.0711],\n",
      "        [0.7026, 0.6956, 0.0478, 0.7714, 0.1275],\n",
      "        [0.0265, 0.9607, 0.0190, 0.4322, 0.2312],\n",
      "        [0.0100, 0.2980, 0.2941, 0.0992, 0.0618],\n",
      "        [0.0264, 0.8178, 0.0730, 0.0600, 0.7455],\n",
      "        [0.0487, 0.8588, 0.0429, 0.6406, 0.0208],\n",
      "        [0.0127, 0.5563, 0.1392, 0.1005, 0.0438],\n",
      "        [0.0693, 0.8553, 0.0445, 0.8323, 0.0338],\n",
      "        [0.4572, 0.9119, 0.0245, 0.3736, 0.4298],\n",
      "        [0.1545, 0.4700, 0.1512, 0.3685, 0.0608],\n",
      "        [0.2258, 0.4892, 0.0981, 0.3077, 0.1991],\n",
      "        [0.0184, 0.0883, 0.7161, 0.0465, 0.0961],\n",
      "        [0.0980, 0.9628, 0.0131, 0.6115, 0.1276],\n",
      "        [0.1364, 0.6571, 0.0699, 0.5088, 0.0415],\n",
      "        [0.0251, 0.1530, 0.6081, 0.0569, 0.3410],\n",
      "        [0.1945, 0.4251, 0.1882, 0.1148, 0.6352],\n",
      "        [0.0851, 0.5394, 0.1845, 0.7926, 0.0341],\n",
      "        [0.0163, 0.1065, 0.7032, 0.0533, 0.0796],\n",
      "        [0.2624, 0.9387, 0.0151, 0.5157, 0.3989],\n",
      "        [0.0140, 0.6482, 0.1042, 0.0976, 0.0513],\n",
      "        [0.0620, 0.8972, 0.0279, 0.2445, 0.2940],\n",
      "        [0.0081, 0.5640, 0.1767, 0.1003, 0.0754],\n",
      "        [0.0688, 0.8929, 0.0348, 0.8282, 0.0261],\n",
      "        [0.0231, 0.8736, 0.0428, 0.2667, 0.0412],\n",
      "        [0.1407, 0.7954, 0.0851, 0.0983, 0.8855],\n",
      "        [0.0329, 0.8219, 0.0574, 0.5897, 0.0166],\n",
      "        [0.0252, 0.8722, 0.0420, 0.2468, 0.0429],\n",
      "        [0.0745, 0.0627, 0.8709, 0.1094, 0.0987],\n",
      "        [0.0345, 0.1442, 0.6150, 0.0781, 0.1504],\n",
      "        [0.1769, 0.8343, 0.0559, 0.1184, 0.8300],\n",
      "        [0.0351, 0.3098, 0.2271, 0.1096, 0.2457],\n",
      "        [0.1094, 0.8915, 0.0345, 0.8555, 0.1963],\n",
      "        [0.0534, 0.0754, 0.8243, 0.0555, 0.3149],\n",
      "        [0.0657, 0.0660, 0.8483, 0.1514, 0.0970],\n",
      "        [0.1917, 0.8298, 0.0619, 0.1535, 0.8878],\n",
      "        [0.0172, 0.1435, 0.5857, 0.0566, 0.1021],\n",
      "        [0.0912, 0.9250, 0.0211, 0.7660, 0.0340],\n",
      "        [0.0660, 0.9411, 0.0217, 0.7434, 0.0339],\n",
      "        [0.1207, 0.9747, 0.0133, 0.6066, 0.3703],\n",
      "        [0.0304, 0.8085, 0.0447, 0.3136, 0.2301],\n",
      "        [0.0468, 0.5247, 0.1694, 0.0629, 0.8446],\n",
      "        [0.2464, 0.9006, 0.0257, 0.9000, 0.0894],\n",
      "        [0.0320, 0.1231, 0.7439, 0.0430, 0.4450],\n",
      "        [0.2636, 0.8506, 0.0380, 0.9124, 0.1072],\n",
      "        [0.6168, 0.5308, 0.1091, 0.3915, 0.3127],\n",
      "        [0.0667, 0.9034, 0.0221, 0.7973, 0.0375],\n",
      "        [0.0270, 0.9573, 0.0214, 0.4627, 0.1020],\n",
      "        [0.2993, 0.7883, 0.0349, 0.3934, 0.1612],\n",
      "        [0.1475, 0.9605, 0.0187, 0.4426, 0.6328],\n",
      "        [0.2813, 0.9562, 0.0158, 0.4760, 0.4999],\n",
      "        [0.7773, 0.7010, 0.0547, 0.7566, 0.1506],\n",
      "        [0.0921, 0.2214, 0.3970, 0.2747, 0.0407],\n",
      "        [0.3705, 0.8221, 0.0518, 0.1888, 0.7843],\n",
      "        [0.0209, 0.7657, 0.0635, 0.1225, 0.2453],\n",
      "        [0.0389, 0.9428, 0.0242, 0.6962, 0.0404],\n",
      "        [0.0121, 0.5855, 0.1383, 0.0679, 0.1036],\n",
      "        [0.0158, 0.7512, 0.0840, 0.0561, 0.2912],\n",
      "        [0.0136, 0.3085, 0.3219, 0.0294, 0.2193],\n",
      "        [0.0129, 0.1562, 0.5843, 0.0671, 0.0671],\n",
      "        [0.5841, 0.6219, 0.0854, 0.3298, 0.3008],\n",
      "        [0.0177, 0.9279, 0.0294, 0.2129, 0.3317],\n",
      "        [0.0124, 0.5146, 0.1523, 0.1614, 0.1174],\n",
      "        [0.0216, 0.0974, 0.7592, 0.0469, 0.3150],\n",
      "        [0.0472, 0.6406, 0.1166, 0.7297, 0.0287],\n",
      "        [0.0185, 0.5039, 0.1727, 0.2257, 0.0192],\n",
      "        [0.5660, 0.5704, 0.1155, 0.8373, 0.0739],\n",
      "        [0.7150, 0.6781, 0.0629, 0.7494, 0.1432],\n",
      "        [0.0137, 0.5153, 0.1574, 0.0968, 0.0476],\n",
      "        [0.1024, 0.8137, 0.0697, 0.0722, 0.8195],\n",
      "        [0.0440, 0.7884, 0.1054, 0.0640, 0.8920],\n",
      "        [0.0409, 0.8470, 0.0530, 0.7242, 0.0184],\n",
      "        [0.0836, 0.3162, 0.2749, 0.5980, 0.0352],\n",
      "        [0.0661, 0.9169, 0.0330, 0.1965, 0.8342]])\n",
      "tensor([[1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [1, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [1, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 0, 1],\n",
      "        [0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# predictions = trainer.predict(tokenized_dev) # logits\n",
    "# probs = torch.sigmoid(torch.from_numpy(predictions.predictions)) # percentage probabilities\n",
    "# print(\"PROBS: \", probs)\n",
    "# binary_predictions = (probs >= 0.35).long()\n",
    "# print(binary_predictions)\n",
    "\n",
    "# # Convert tensors to integers\n",
    "# binary_predictions_list = binary_predictions.tolist()\n",
    "\n",
    "\n",
    "# ids = []\n",
    "# for i in range(0, len(binary_predictions_list)):\n",
    "#   ids.append(\"text\" + str(i))\n",
    "\n",
    "# data = {\n",
    "#     \"id\": ids,\n",
    "#     \"Anger\": [pred[0] for pred in binary_predictions_list],\n",
    "#     \"Fear\": [pred[1] for pred in binary_predictions_list],\n",
    "#     \"Joy\": [pred[2] for pred in binary_predictions_list],\n",
    "#     \"Sadness\": [pred[3] for pred in binary_predictions_list],\n",
    "#     \"Surprise\": [pred[4] for pred in binary_predictions_list],\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv(\"pred_eng_a.csv\", index=False) # drop the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semeval1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
